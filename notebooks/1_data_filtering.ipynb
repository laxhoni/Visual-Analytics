{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b891181d",
   "metadata": {},
   "source": [
    "# 1. Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13fcdcd",
   "metadata": {},
   "source": [
    "* Objetivo: Crear un \"Ground Truth\" de texto humano puro.\n",
    "\n",
    "* Estrategia: Leer un dataset masivo de noticias y extraer una muestra aleatoria de art√≠culos publicados antes de 2020 (antes del auge de GPT-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffcfca",
   "metadata": {},
   "source": [
    "## 1.1. Configuraci√≥n inicial\n",
    "\n",
    "En esta secci√≥n importamos las librer√≠as necesarias para la manipulaci√≥n de datos (`pandas`) y gesti√≥n del sistema (`os`). \n",
    "\n",
    "Definimos constantes cr√≠ticas:\n",
    "* `INPUT_FILE`: Ruta al dataset masivo descargado de Kaggle.\n",
    "* `CHUNK_SIZE`: Tama√±o del bloque de lectura (50,000 filas) para evitar desbordar la memoria RAM.\n",
    "* `DATE_CUTOFF`: La fecha l√≠mite (**01-01-2020**) que act√∫a como frontera para considerar un texto como \"100% Humano\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Configuraci√≥n de rutas y par√°metros\n",
    "INPUT_FILE = '../data/1_raw/all-the-news-2-1.csv' \n",
    "\n",
    "# Ruta donde guardaremos la muestra peque√±a\n",
    "OUTPUT_FILE =  '../data/1_raw/all-the-news-sample.csv' \n",
    "\n",
    "# Par√°metros de muestreo\n",
    "SAMPLE_SIZE = 5000       \n",
    "CHUNK_SIZE = 50000       \n",
    "DATE_CUTOFF = '2020-01-01' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e718c221",
   "metadata": {},
   "source": [
    "## 1.2. Funci√≥n principal de filtrado y muestreo\n",
    "\n",
    "Aqu√≠ definimos la l√≥gica central del script (`create_sample`). Dado que el archivo original pesa ~8GB, no podemos cargarlo entero.\n",
    "\n",
    "**El algoritmo funciona as√≠:**\n",
    "1.  **Iteraci√≥n:** Recorre el archivo CSV original bloque a bloque (*chunk by chunk*).\n",
    "2.  **Filtrado:** En cada bloque, descarta las noticias posteriores a 2020 y aquellas sin contenido v√°lido.\n",
    "3.  **Acumulaci√≥n:** Guarda temporalmente los candidatos v√°lidos.\n",
    "4.  **Muestreo:** Una vez recorrido todo el archivo, selecciona aleatoriamente `SAMPLE_SIZE` (5,000) art√≠culos para garantizar representatividad estad√≠stica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample():\n",
    "    \"\"\"\n",
    "    Lee el archivo masivo por bloques, filtra noticias antiguas y selecciona\n",
    "    una muestra aleatoria para el estudio.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lista temporal para guardar candidatos v√°lidos\n",
    "    valid_articles_pool = []\n",
    "    \n",
    "    # Procesamos el archivo por trozos (chunks)\n",
    "    try:\n",
    "        chunks = pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE, parse_dates=['date'], low_memory=False)\n",
    "        \n",
    "        total_processed = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # 1. Limpieza b√°sica de fechas nulas\n",
    "            chunk = chunk.dropna(subset=['date', 'article'])\n",
    "            \n",
    "            # 2. Convertir fecha a datetime (por seguridad, si parse_dates falla)\n",
    "            chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n",
    "            \n",
    "            # Nos aseguramos de que sea anterior a 2020 Y que no sea muy antigua (ej. 2015 para modernidad)\n",
    "            mask = (chunk['date'] < DATE_CUTOFF) & (chunk['date'] >= '2015-01-01')\n",
    "            valid_chunk = chunk[mask]\n",
    "            \n",
    "            # 4. Optimizaci√≥n: Guardamos solo columnas necesarias para ahorrar memoria\n",
    "            cols_to_keep = ['date', 'title', 'article', 'publication', 'author']\n",
    "            \n",
    "            # Aseguramos que existan las columnas antes de filtrar\n",
    "            existing_cols = [c for c in cols_to_keep if c in valid_chunk.columns]\n",
    "            \n",
    "            valid_articles_pool.append(valid_chunk[existing_cols])\n",
    "            \n",
    "            total_processed += len(chunk)\n",
    "            print(f\"   ‚è≥ Procesadas {total_processed:,} filas... (Encontrados {len(valid_chunk)} v√°lidos en este bloque)\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error leyendo el archivo: {e}\")\n",
    "        return\n",
    "\n",
    "    # Concatenar todos los trozos v√°lidos en un solo DataFrame temporal\n",
    "    full_valid_df = pd.concat(valid_articles_pool, ignore_index=True)\n",
    "    \n",
    "    print(f\"Total de art√≠culos v√°lidos (Pre-2020) encontrados: {len(full_valid_df):,}\")\n",
    "\n",
    "    # 5. Muestreo aleatorio\n",
    "    if len(full_valid_df) > SAMPLE_SIZE:\n",
    "        print(f\"üé≤ Seleccionando aleatoriamente {SAMPLE_SIZE} noticias...\")\n",
    "        sampled_df = full_valid_df.sample(n=SAMPLE_SIZE, random_state=42) # random_state para reproducibilidad\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Advertencia: Hay menos de {SAMPLE_SIZE} art√≠culos. Usando todos los disponibles.\")\n",
    "        sampled_df = full_valid_df\n",
    "\n",
    "    # 6. Guardardamos el DataFrame muestreado a CSV\n",
    "    print(f\"Guardando CSV en {OUTPUT_FILE}...\")\n",
    "    sampled_df.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a64bf",
   "metadata": {},
   "source": [
    "## 1.3. Ejecuci√≥n del Script\n",
    "\n",
    "Bloque de entrada est√°ndar. Ejecuta la funci√≥n `create_sample()` definida anteriormente. \n",
    "Si el archivo de salida ya existe, este proceso lo sobrescribir√° con una nueva muestra aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250311b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
