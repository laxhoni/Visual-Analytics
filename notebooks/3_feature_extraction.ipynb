{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdc45fe",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering (Extracci√≥n de Caracter√≠sticas)\n",
    "\n",
    "**Objetivo:** Transformar el texto crudo en un conjunto de m√©tricas num√©ricas (Dataset Procesado).\n",
    "\n",
    "**Estrategia:** Unificar el dataset Humano y el dataset AI en un solo DataFrame. Luego, aplicar t√©cnicas de NLP para medir la riqueza de vocabulario, complejidad sint√°ctica, sentimiento y legibilidad.\n",
    "\n",
    "El resultado ser√° un archivo `.csv` donde cada fila es un art√≠culo y cada columna es una m√©trica matem√°tica, listo para entrenar el modelo XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019878e9",
   "metadata": {},
   "source": [
    "## 3.1. Configuraci√≥n e importaci√≥n de librer√≠as\n",
    "\n",
    "Importamos las herramientas necesarias. Usaremos `spaCy` como motor principal para el an√°lisis sint√°ctico (gram√°tica profunda) debido a su precisi√≥n y velocidad. Tambi√©n gestionamos la carga del modelo de idioma ingl√©s (`en_core_web_sm`).\n",
    "\n",
    "Importamos `pandas` para datos, `textblob` para an√°lisis de sentimiento, `textstat` para m√©tricas de legibilidad y `nltk` para tokenizaci√≥n avanzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d44d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b546eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo spaCy cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import spacy \n",
    "import os\n",
    "\n",
    "# Cargamos el modelo de ingl√©s de spaCy\n",
    "# (Es mucho m√°s r√°pido y preciso que NLTK para gram√°tica)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ Modelo spaCy cargado correctamente.\")\n",
    "except OSError:\n",
    "    print(\"‚ùå ERROR: No tienes el modelo descargado.\")\n",
    "    print(\"Ejecuta en tu terminal: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# --- RUTAS ---\n",
    "HUMAN_FILE = '../data/1_raw/all-the-news-5k-sample.csv'\n",
    "AI_FILE = '../data/3_synthetic/ai_generated_gemini.csv'\n",
    "OUTPUT_FILE = '../data/2_processed/training_data_final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b895d1c",
   "metadata": {},
   "source": [
    "## 3.2. Definici√≥n de la Funci√≥n de Extracci√≥n (El Motor NLP)\n",
    "\n",
    "Esta funci√≥n es el n√∫cleo del proyecto. Recibe un texto crudo y devuelve un diccionario con sus \"signos vitales\" ling√º√≠sticos:\n",
    "\n",
    "1.  **Diversidad L√©xica:** Relaci√≥n entre palabras √∫nicas y total de palabras (riqueza de vocabulario).\n",
    "2.  **Densidad Gramatical:** Proporci√≥n de palabras de contenido (verbos, sustantivos, adjetivos) frente a palabras funcionales.\n",
    "3.  **Ratio de Adjetivos:** Las IAs a veces tienden a usar un lenguaje m√°s \"florido\" o descriptivo.\n",
    "4.  **Sentimiento:** Polaridad (positivo/negativo) y Subjetividad.\n",
    "5.  **Legibilidad:** Nivel educativo necesario para entender el texto (Flesch-Kincaid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e553398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extrae m√©tricas avanzadas incluyendo Varianza y Estilo.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return None \n",
    "        \n",
    "    # 1. Procesamiento spaCy\n",
    "    doc = nlp(text)\n",
    "    num_tokens = len(doc)\n",
    "    if num_tokens == 0: return None\n",
    "    \n",
    "    # Lista de longitudes de cada frase (para calcular varianza)\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    num_sentences = len(sentence_lengths)\n",
    "    \n",
    "    # 2. M√©tricas Gramaticales (POS Tagging)\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    num_verbs = pos_counts.get(spacy.symbols.VERB, 0)\n",
    "    num_adjs = pos_counts.get(spacy.symbols.ADJ, 0)\n",
    "    num_nouns = pos_counts.get(spacy.symbols.NOUN, 0)\n",
    "    num_nums = pos_counts.get(spacy.symbols.NUM, 0) # üî• NUEVO: Cifras/N√∫meros\n",
    "    \n",
    "    # 3. üî• NUEVO: M√©tricas de \"Humanidad\" (Caos y Contenido)\n",
    "    \n",
    "    # Stopwords (Palabras de relleno)\n",
    "    num_stopwords = sum(1 for token in doc if token.is_stop)\n",
    "    stopword_ratio = num_stopwords / num_tokens\n",
    "    \n",
    "    # Desviaci√≥n Est√°ndar de frases (Burstiness Proxy)\n",
    "    # Si es 0 o bajo = Robot mon√≥tono. Si es alto = Humano ca√≥tico.\n",
    "    sent_len_std = np.std(sentence_lengths) if num_sentences > 1 else 0\n",
    "    \n",
    "    # Ratio de Entidades Nombradas (Personas, Pa√≠ses, Empresas)\n",
    "    # El periodismo humano suele tener muchas entidades espec√≠ficas.\n",
    "    num_entities = len(doc.ents)\n",
    "    entity_ratio = num_entities / num_tokens\n",
    "    \n",
    "    # Riqueza l√©xica\n",
    "    unique_words = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "    lexical_diversity = unique_words / num_tokens if num_tokens > 0 else 0\n",
    "    \n",
    "    # 4. Sentimiento y Legibilidad\n",
    "    blob = TextBlob(text)\n",
    "    reading_ease = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    return {\n",
    "        # --- Estructura ---\n",
    "        'word_count': num_tokens,\n",
    "        'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0,\n",
    "        'sent_len_std': sent_len_std,       # üî• CLAVE: Varianza de estructura\n",
    "        \n",
    "        # --- Estilo ---\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'stopword_ratio': stopword_ratio,   # üî• CLAVE: \"Fluff\" ratio\n",
    "        'adj_ratio': num_adjs / num_tokens,\n",
    "        'verb_ratio': num_verbs / num_tokens,\n",
    "        'noun_ratio': num_nouns / num_tokens, # üî• CLAVE: Densidad de informaci√≥n\n",
    "        'entity_ratio': entity_ratio,         # üî• CLAVE: Anclaje en la realidad\n",
    "        \n",
    "        # --- Sem√°ntica ---\n",
    "        'sentiment_polarity': blob.sentiment.polarity,\n",
    "        'sentiment_subjectivity': blob.sentiment.subjectivity,\n",
    "        'reading_ease': reading_ease\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a905b",
   "metadata": {},
   "source": [
    "## 3.3. Carga y Unificaci√≥n de Datos\n",
    "\n",
    "1.  Cargamos el CSV Humano y le asignamos la etiqueta `label = 0`.\n",
    "2.  Cargamos el CSV de IA y le asignamos la etiqueta `label = 1`.\n",
    "3.  Los fusionamos en un solo DataFrame grande (`df_full`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2daf5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando datasets con trazabilidad...\n",
      "   -> Humanos: 5000 filas.\n",
      "   -> IA Generada: 4998 filas.\n",
      "üìä Dataset Consolidado: 9998 filas.\n",
      "   Ejemplo de trazabilidad (ID 0):\n",
      "      label  original_id\n",
      "0         0            0\n",
      "5000      1            0\n"
     ]
    }
   ],
   "source": [
    "## 3.3. Carga y Unificaci√≥n de Datos (Con Trazabilidad)\n",
    "# ------------------------------------------\n",
    "print(\"üìÇ Cargando datasets con trazabilidad...\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# 1. Cargar Humano\n",
    "if os.path.exists(HUMAN_FILE):\n",
    "    df_h = pd.read_csv(HUMAN_FILE)\n",
    "    df_h['label'] = 0\n",
    "    \n",
    "    # CREAMOS EL ID: Usamos el √≠ndice como identificador √∫nico original\n",
    "    # Esto nos permitir√° decir: \"Este texto humano es el padre del texto IA X\"\n",
    "    df_h['original_id'] = df_h.index\n",
    "    \n",
    "    # Seleccionamos columnas clave\n",
    "    if 'content' in df_h.columns: df_h.rename(columns={'content': 'article'}, inplace=True)\n",
    "    \n",
    "    # Nos quedamos con ID, Texto y Etiqueta\n",
    "    dfs.append(df_h[['original_id', 'article', 'label']].dropna())\n",
    "    print(f\"   -> Humanos: {len(df_h)} filas.\")\n",
    "else:\n",
    "    print(\"‚ùå Falta archivo Humano.\")\n",
    "\n",
    "# 2. Cargar IA\n",
    "if os.path.exists(AI_FILE):\n",
    "    df_a = pd.read_csv(AI_FILE)\n",
    "    df_a['label'] = 1\n",
    "    \n",
    "    # Aqu√≠ 'original_id' YA VIENE del Notebook 2 (¬°tu gran acierto!)\n",
    "    # Solo nos aseguramos de tener las mismas columnas\n",
    "    dfs.append(df_a[['original_id', 'article', 'label']].dropna())\n",
    "    print(f\"   -> IA Generada: {len(df_a)} filas.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AVISO: A√∫n no existe el archivo de IA.\")\n",
    "\n",
    "# 3. Fusi√≥n Inteligente\n",
    "if not dfs:\n",
    "    print(\"üõë No hay datos.\")\n",
    "else:\n",
    "    df_full = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Verificaci√≥n de integridad\n",
    "    # Si el ID 5 existe en Humano pero no en IA (porque fall√≥), no pasa nada.\n",
    "    # Pero ahora tendremos el dato guardado.\n",
    "    print(f\"üìä Dataset Consolidado: {len(df_full)} filas.\")\n",
    "    print(f\"   Ejemplo de trazabilidad (ID 0):\")\n",
    "    print(df_full[df_full['original_id'] == 0][['label', 'original_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00709fd",
   "metadata": {},
   "source": [
    "## 3.4. Ejecuci√≥n del procesado y guardado\n",
    "\n",
    "Iteramos sobre cada fila del dataset combinado, aplicamos la funci√≥n `extract_features` y construimos el DataFrame final.\n",
    "Este proceso puede tardar unos minutos dependiendo de la cantidad de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059b9e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Extrayendo caracter√≠sticas...\n",
      "   ... 250 procesados\n",
      "   ... 500 procesados\n",
      "   ... 750 procesados\n",
      "   ... 1000 procesados\n",
      "   ... 1250 procesados\n",
      "   ... 1500 procesados\n",
      "   ... 1750 procesados\n",
      "   ... 2000 procesados\n",
      "   ... 2250 procesados\n",
      "   ... 2500 procesados\n",
      "   ... 2750 procesados\n",
      "   ... 3000 procesados\n",
      "   ... 3250 procesados\n",
      "   ... 3500 procesados\n",
      "   ... 3750 procesados\n",
      "   ... 4000 procesados\n",
      "   ... 4250 procesados\n",
      "   ... 4500 procesados\n",
      "   ... 4750 procesados\n",
      "   ... 5000 procesados\n",
      "   ... 5250 procesados\n",
      "   ... 5500 procesados\n",
      "   ... 5750 procesados\n",
      "   ... 6000 procesados\n",
      "   ... 6250 procesados\n",
      "   ... 6500 procesados\n",
      "   ... 6750 procesados\n",
      "   ... 7000 procesados\n",
      "   ... 7250 procesados\n",
      "   ... 7500 procesados\n",
      "   ... 7750 procesados\n",
      "   ... 8000 procesados\n",
      "   ... 8250 procesados\n",
      "   ... 8500 procesados\n",
      "   ... 8750 procesados\n",
      "   ... 9000 procesados\n",
      "   ... 9250 procesados\n",
      "   ... 9500 procesados\n",
      "   ... 9750 procesados\n",
      "------------------------------\n",
      "‚úÖ ¬°√âXITO! Dataset final guardado en: ../data/2_processed/training_data_final.csv\n",
      "   original_id  label  word_count  avg_sentence_length  sent_len_std  \\\n",
      "0            0      0         189            23.625000      9.860749   \n",
      "1            1      0          65            16.250000      5.068284   \n",
      "2            2      0         180            22.500000     13.190906   \n",
      "3            3      0        1061            25.261905     10.801366   \n",
      "4            4      0         463            25.722222     12.390882   \n",
      "\n",
      "   lexical_diversity  stopword_ratio  adj_ratio  verb_ratio  noun_ratio  \\\n",
      "0           0.502646        0.476190   0.037037    0.142857    0.121693   \n",
      "1           0.646154        0.184615   0.076923    0.076923    0.276923   \n",
      "2           0.533333        0.450000   0.072222    0.111111    0.238889   \n",
      "3           0.323280        0.342130   0.046183    0.100848    0.196984   \n",
      "4           0.473002        0.434125   0.058315    0.112311    0.200864   \n",
      "\n",
      "   entity_ratio  sentiment_polarity  sentiment_subjectivity  reading_ease  \n",
      "0      0.052910            0.326000                0.568000     74.187440  \n",
      "1      0.061538            0.045455                0.318182    -24.141132  \n",
      "2      0.055556            0.166176                0.511275     54.894094  \n",
      "3      0.163054            0.020428                0.316889     64.706490  \n",
      "4      0.077754            0.100553                0.537968     52.866780  \n"
     ]
    }
   ],
   "source": [
    "## 3.4. Procesado Masivo (Preservando IDs)\n",
    "# ------------------------------------------\n",
    "print(\"‚öôÔ∏è Extrayendo caracter√≠sticas...\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for index, row in df_full.iterrows():\n",
    "    metrics = extract_features(row['article'])\n",
    "    \n",
    "    if metrics:\n",
    "        # 1. Guardamos la etiqueta (Target)\n",
    "        metrics['label'] = row['label']\n",
    "        \n",
    "        # 2. Guardamos el ID ORIGINAL (Trazabilidad)\n",
    "        metrics['original_id'] = row['original_id']\n",
    "        \n",
    "        features_list.append(metrics)\n",
    "    \n",
    "    if (index + 1) % 250 == 0:\n",
    "        print(f\"   ... {index + 1} procesados\")\n",
    "\n",
    "# Guardado Final\n",
    "df_final = pd.DataFrame(features_list)\n",
    "\n",
    "# REORDENAR COLUMNAS: Poner 'original_id' y 'label' al principio para que sea legible\n",
    "cols = ['original_id', 'label'] + [c for c in df_final.columns if c not in ['original_id', 'label']]\n",
    "df_final = df_final[cols]\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Dataset final guardado en: {OUTPUT_FILE}\")\n",
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7f8381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>sent_len_std</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>stopword_ratio</th>\n",
       "      <th>adj_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>23.625000</td>\n",
       "      <td>9.860749</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>74.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>5.068284</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>-24.141132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.238889</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.166176</td>\n",
       "      <td>0.511275</td>\n",
       "      <td>54.894094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1061</td>\n",
       "      <td>25.261905</td>\n",
       "      <td>10.801366</td>\n",
       "      <td>0.323280</td>\n",
       "      <td>0.342130</td>\n",
       "      <td>0.046183</td>\n",
       "      <td>0.100848</td>\n",
       "      <td>0.196984</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>0.020428</td>\n",
       "      <td>0.316889</td>\n",
       "      <td>64.706490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>463</td>\n",
       "      <td>25.722222</td>\n",
       "      <td>12.390882</td>\n",
       "      <td>0.473002</td>\n",
       "      <td>0.434125</td>\n",
       "      <td>0.058315</td>\n",
       "      <td>0.112311</td>\n",
       "      <td>0.200864</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.100553</td>\n",
       "      <td>0.537968</td>\n",
       "      <td>52.866780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_id  label  word_count  avg_sentence_length  sent_len_std  \\\n",
       "0            0      0         189            23.625000      9.860749   \n",
       "1            1      0          65            16.250000      5.068284   \n",
       "2            2      0         180            22.500000     13.190906   \n",
       "3            3      0        1061            25.261905     10.801366   \n",
       "4            4      0         463            25.722222     12.390882   \n",
       "\n",
       "   lexical_diversity  stopword_ratio  adj_ratio  verb_ratio  noun_ratio  \\\n",
       "0           0.502646        0.476190   0.037037    0.142857    0.121693   \n",
       "1           0.646154        0.184615   0.076923    0.076923    0.276923   \n",
       "2           0.533333        0.450000   0.072222    0.111111    0.238889   \n",
       "3           0.323280        0.342130   0.046183    0.100848    0.196984   \n",
       "4           0.473002        0.434125   0.058315    0.112311    0.200864   \n",
       "\n",
       "   entity_ratio  sentiment_polarity  sentiment_subjectivity  reading_ease  \n",
       "0      0.052910            0.326000                0.568000     74.187440  \n",
       "1      0.061538            0.045455                0.318182    -24.141132  \n",
       "2      0.055556            0.166176                0.511275     54.894094  \n",
       "3      0.163054            0.020428                0.316889     64.706490  \n",
       "4      0.077754            0.100553                0.537968     52.866780  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207e48d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>sent_len_std</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>stopword_ratio</th>\n",
       "      <th>adj_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>4995</td>\n",
       "      <td>1</td>\n",
       "      <td>638</td>\n",
       "      <td>27.739130</td>\n",
       "      <td>8.414363</td>\n",
       "      <td>0.536050</td>\n",
       "      <td>0.315047</td>\n",
       "      <td>0.108150</td>\n",
       "      <td>0.117555</td>\n",
       "      <td>0.278997</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>0.053511</td>\n",
       "      <td>0.459148</td>\n",
       "      <td>12.771626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>4996</td>\n",
       "      <td>1</td>\n",
       "      <td>935</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>10.564229</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>0.037433</td>\n",
       "      <td>0.094531</td>\n",
       "      <td>0.513755</td>\n",
       "      <td>16.833576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>4997</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>41.333333</td>\n",
       "      <td>9.177267</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.104839</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.043939</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>22.452571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>4998</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>22.681818</td>\n",
       "      <td>10.301749</td>\n",
       "      <td>0.527054</td>\n",
       "      <td>0.298597</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>0.266533</td>\n",
       "      <td>0.062124</td>\n",
       "      <td>0.078184</td>\n",
       "      <td>0.445764</td>\n",
       "      <td>23.353770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "      <td>494</td>\n",
       "      <td>32.933333</td>\n",
       "      <td>11.783982</td>\n",
       "      <td>0.485830</td>\n",
       "      <td>0.408907</td>\n",
       "      <td>0.058704</td>\n",
       "      <td>0.143725</td>\n",
       "      <td>0.170040</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.087079</td>\n",
       "      <td>0.468913</td>\n",
       "      <td>28.104177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      original_id  label  word_count  avg_sentence_length  sent_len_std  \\\n",
       "9992         4995      1         638            27.739130      8.414363   \n",
       "9993         4996      1         935            27.500000     10.564229   \n",
       "9994         4997      1         124            41.333333      9.177267   \n",
       "9995         4998      1         499            22.681818     10.301749   \n",
       "9996         4999      1         494            32.933333     11.783982   \n",
       "\n",
       "      lexical_diversity  stopword_ratio  adj_ratio  verb_ratio  noun_ratio  \\\n",
       "9992           0.536050        0.315047   0.108150    0.117555    0.278997   \n",
       "9993           0.505882        0.352941   0.122995    0.109091    0.204278   \n",
       "9994           0.661290        0.330645   0.104839    0.112903    0.225806   \n",
       "9995           0.527054        0.298597   0.098196    0.096192    0.266533   \n",
       "9996           0.485830        0.408907   0.058704    0.143725    0.170040   \n",
       "\n",
       "      entity_ratio  sentiment_polarity  sentiment_subjectivity  reading_ease  \n",
       "9992      0.032915            0.053511                0.459148     12.771626  \n",
       "9993      0.037433            0.094531                0.513755     16.833576  \n",
       "9994      0.080645            0.043939                0.357576     22.452571  \n",
       "9995      0.062124            0.078184                0.445764     23.353770  \n",
       "9996      0.052632            0.087079                0.468913     28.104177  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_final.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
