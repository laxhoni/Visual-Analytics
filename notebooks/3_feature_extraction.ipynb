{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdc45fe",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering (Extracci√≥n de Caracter√≠sticas)\n",
    "\n",
    "**Objetivo:** Transformar el texto crudo en un conjunto de m√©tricas num√©ricas (Dataset Procesado).\n",
    "\n",
    "**Estrategia:** Unificar el dataset Humano y el dataset AI en un solo DataFrame. Luego, aplicar t√©cnicas de NLP para medir la riqueza de vocabulario, complejidad sint√°ctica, sentimiento y legibilidad.\n",
    "\n",
    "El resultado ser√° un archivo `.csv` donde cada fila es un art√≠culo y cada columna es una m√©trica matem√°tica, listo para entrenar el modelo XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019878e9",
   "metadata": {},
   "source": [
    "## 3.1. Configuraci√≥n e importaci√≥n de librer√≠as\n",
    "\n",
    "Importamos las herramientas necesarias. Usaremos `spaCy` como motor principal para el an√°lisis sint√°ctico (gram√°tica profunda) debido a su precisi√≥n y velocidad. Tambi√©n gestionamos la carga del modelo de idioma ingl√©s (`en_core_web_sm`).\n",
    "\n",
    "Importamos `pandas` para datos, `textblob` para an√°lisis de sentimiento, `textstat` para m√©tricas de legibilidad y `nltk` para tokenizaci√≥n avanzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\barco\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\barco\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\barco\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\barco\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 5.3 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.4/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.6/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 5.0/12.8 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.6 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#%pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48b546eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo spaCy cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import spacy \n",
    "import os\n",
    "\n",
    "# Cargamos el modelo de ingl√©s de spaCy\n",
    "# (Es mucho m√°s r√°pido y preciso que NLTK para gram√°tica)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ Modelo spaCy cargado correctamente.\")\n",
    "except OSError:\n",
    "    print(\"‚ùå ERROR: No tienes el modelo descargado.\")\n",
    "    print(\"Ejecuta en tu terminal: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# --- RUTAS ---\n",
    "HUMAN_FILE = '../data/1_raw/all-the-news-5k-sample.csv'\n",
    "AI_FILE = '../data/3_synthetic/ai_generated_gemini.csv'\n",
    "OUTPUT_FILE = '../data/2_processed/training_data_final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b895d1c",
   "metadata": {},
   "source": [
    "## 3.2. Definici√≥n de la Funci√≥n de Extracci√≥n (El Motor NLP)\n",
    "\n",
    "Esta funci√≥n es el n√∫cleo del proyecto. Recibe un texto crudo y devuelve un diccionario con sus \"signos vitales\" ling√º√≠sticos:\n",
    "\n",
    "1.  **Diversidad L√©xica:** Relaci√≥n entre palabras √∫nicas y total de palabras (riqueza de vocabulario).\n",
    "2.  **Densidad Gramatical:** Proporci√≥n de palabras de contenido (verbos, sustantivos, adjetivos) frente a palabras funcionales.\n",
    "3.  **Ratio de Adjetivos:** Las IAs a veces tienden a usar un lenguaje m√°s \"florido\" o descriptivo.\n",
    "4.  **Sentimiento:** Polaridad (positivo/negativo) y Subjetividad.\n",
    "5.  **Legibilidad:** Nivel educativo necesario para entender el texto (Flesch-Kincaid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e553398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extrae m√©tricas avanzadas incluyendo Varianza y Estilo.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return None \n",
    "        \n",
    "    # 1. Procesamiento spaCy\n",
    "    doc = nlp(text)\n",
    "    num_tokens = len(doc)\n",
    "    if num_tokens == 0: return None\n",
    "    \n",
    "    # Lista de longitudes de cada frase (para calcular varianza)\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    num_sentences = len(sentence_lengths)\n",
    "    \n",
    "    # 2. M√©tricas Gramaticales (POS Tagging)\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    num_verbs = pos_counts.get(spacy.symbols.VERB, 0)\n",
    "    num_adjs = pos_counts.get(spacy.symbols.ADJ, 0)\n",
    "    num_nouns = pos_counts.get(spacy.symbols.NOUN, 0)\n",
    "    num_nums = pos_counts.get(spacy.symbols.NUM, 0) # üî• NUEVO: Cifras/N√∫meros\n",
    "    \n",
    "    # 3. üî• NUEVO: M√©tricas de \"Humanidad\" (Caos y Contenido)\n",
    "    \n",
    "    # Stopwords (Palabras de relleno)\n",
    "    num_stopwords = sum(1 for token in doc if token.is_stop)\n",
    "    stopword_ratio = num_stopwords / num_tokens\n",
    "    \n",
    "    # Desviaci√≥n Est√°ndar de frases (Burstiness Proxy)\n",
    "    # Si es 0 o bajo = Robot mon√≥tono. Si es alto = Humano ca√≥tico.\n",
    "    sent_len_std = np.std(sentence_lengths) if num_sentences > 1 else 0\n",
    "    \n",
    "    # Ratio de Entidades Nombradas (Personas, Pa√≠ses, Empresas)\n",
    "    # El periodismo humano suele tener muchas entidades espec√≠ficas.\n",
    "    num_entities = len(doc.ents)\n",
    "    entity_ratio = num_entities / num_tokens\n",
    "    \n",
    "    # Riqueza l√©xica\n",
    "    unique_words = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "    lexical_diversity = unique_words / num_tokens if num_tokens > 0 else 0\n",
    "    \n",
    "    # 4. Sentimiento y Legibilidad\n",
    "    blob = TextBlob(text)\n",
    "    reading_ease = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    return {\n",
    "        # --- Estructura ---\n",
    "        'word_count': num_tokens,\n",
    "        'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0,\n",
    "        'sent_len_std': sent_len_std,       # üî• CLAVE: Varianza de estructura\n",
    "        \n",
    "        # --- Estilo ---\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'stopword_ratio': stopword_ratio,   # üî• CLAVE: \"Fluff\" ratio\n",
    "        'adj_ratio': num_adjs / num_tokens,\n",
    "        'verb_ratio': num_verbs / num_tokens,\n",
    "        'noun_ratio': num_nouns / num_tokens, # üî• CLAVE: Densidad de informaci√≥n\n",
    "        'entity_ratio': entity_ratio,         # üî• CLAVE: Anclaje en la realidad\n",
    "        \n",
    "        # --- Sem√°ntica ---\n",
    "        'sentiment_polarity': blob.sentiment.polarity,\n",
    "        'sentiment_subjectivity': blob.sentiment.subjectivity,\n",
    "        'reading_ease': reading_ease\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a905b",
   "metadata": {},
   "source": [
    "## 3.3. Carga y Unificaci√≥n de Datos\n",
    "\n",
    "1.  Cargamos el CSV Humano y le asignamos la etiqueta `label = 0`.\n",
    "2.  Cargamos el CSV de IA y le asignamos la etiqueta `label = 1`.\n",
    "3.  Los fusionamos en un solo DataFrame grande (`df_full`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.3. Carga y Unificaci√≥n de Datos (Con Trazabilidad)\n",
    "# ------------------------------------------\n",
    "print(\"üìÇ Cargando datasets con trazabilidad...\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# 1. Cargar Humano\n",
    "if os.path.exists(HUMAN_FILE):\n",
    "    df_h = pd.read_csv(HUMAN_FILE)\n",
    "    df_h['label'] = 0\n",
    "    \n",
    "    # CREAMOS EL ID: Usamos el √≠ndice como identificador √∫nico original\n",
    "    # Esto nos permitir√° decir: \"Este texto humano es el padre del texto IA X\"\n",
    "    df_h['original_id'] = df_h.index\n",
    "    \n",
    "    # Seleccionamos columnas clave\n",
    "    if 'content' in df_h.columns: df_h.rename(columns={'content': 'article'}, inplace=True)\n",
    "    \n",
    "    # Nos quedamos con ID, Texto y Etiqueta\n",
    "    dfs.append(df_h[['original_id', 'article', 'label']].dropna())\n",
    "    print(f\"   -> Humanos: {len(df_h)} filas.\")\n",
    "else:\n",
    "    print(\"‚ùå Falta archivo Humano.\")\n",
    "\n",
    "# 2. Cargar IA\n",
    "if os.path.exists(AI_FILE):\n",
    "    df_a = pd.read_csv(AI_FILE)\n",
    "    df_a['label'] = 1\n",
    "    \n",
    "    # Aqu√≠ 'original_id' YA VIENE del Notebook 2 (¬°tu gran acierto!)\n",
    "    # Solo nos aseguramos de tener las mismas columnas\n",
    "    dfs.append(df_a[['original_id', 'article', 'label']].dropna())\n",
    "    print(f\"   -> IA Generada: {len(df_a)} filas.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AVISO: A√∫n no existe el archivo de IA.\")\n",
    "\n",
    "# 3. Fusi√≥n Inteligente\n",
    "if not dfs:\n",
    "    print(\"üõë No hay datos.\")\n",
    "else:\n",
    "    df_full = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Verificaci√≥n de integridad\n",
    "    # Si el ID 5 existe en Humano pero no en IA (porque fall√≥), no pasa nada.\n",
    "    # Pero ahora tendremos el dato guardado.\n",
    "    print(f\"üìä Dataset Consolidado: {len(df_full)} filas.\")\n",
    "    print(f\"   Ejemplo de trazabilidad (ID 0):\")\n",
    "    print(df_full[df_full['original_id'] == 0][['label', 'original_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00709fd",
   "metadata": {},
   "source": [
    "## 3.4. Ejecuci√≥n del procesado y guardado\n",
    "\n",
    "Iteramos sobre cada fila del dataset combinado, aplicamos la funci√≥n `extract_features` y construimos el DataFrame final.\n",
    "Este proceso puede tardar unos minutos dependiendo de la cantidad de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.4. Procesado Masivo (Preservando IDs)\n",
    "# ------------------------------------------\n",
    "print(\"‚öôÔ∏è Extrayendo caracter√≠sticas...\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for index, row in df_full.iterrows():\n",
    "    metrics = extract_features(row['article'])\n",
    "    \n",
    "    if metrics:\n",
    "        # 1. Guardamos la etiqueta (Target)\n",
    "        metrics['label'] = row['label']\n",
    "        \n",
    "        # 2. Guardamos el ID ORIGINAL (Trazabilidad)\n",
    "        metrics['original_id'] = row['original_id']\n",
    "        \n",
    "        features_list.append(metrics)\n",
    "    \n",
    "    if (index + 1) % 250 == 0:\n",
    "        print(f\"   ... {index + 1} procesados\")\n",
    "\n",
    "# Guardado Final\n",
    "df_final = pd.DataFrame(features_list)\n",
    "\n",
    "# REORDENAR COLUMNAS: Poner 'original_id' y 'label' al principio para que sea legible\n",
    "cols = ['original_id', 'label'] + [c for c in df_final.columns if c not in ['original_id', 'label']]\n",
    "df_final = df_final[cols]\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚úÖ ¬°√âXITO! Dataset final guardado en: {OUTPUT_FILE}\")\n",
    "print(df_final.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
